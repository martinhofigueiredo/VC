{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Assigment 2 Estimation of Apparent Motion\n",
    "\n",
    "<p style=\"text-align:left;\">\n",
    "    Jos√© Pedro Cruz\n",
    "    <span style=\"float:right;\">\n",
    "        up201504646\n",
    "    </span>\n",
    "</p>\n",
    "<p style=\"text-align:left;\">\n",
    "    Martinho Figueiredo\n",
    "    <span style=\"float:right;\">\n",
    "        up201506179\n",
    "    </span>\n",
    "</p>\n",
    "<p style=\"text-align:left;\">\n",
    "    Nuno Nascimento\n",
    "    <span style=\"float:right;\">\n",
    "        up201907933\n",
    "    </span>\n",
    "</p>\n",
    "\n",
    "\n",
    "[![GitHub](https://img.shields.io/badge/github-%23121011.svg?style=for-the-badge&logo=github&logoColor=white)](https://github.com/martinhofigueiredo/VC)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scheme\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[Ingest Footage] -->|mp4 or mpegs| B{Multi Channel?}\n",
    "    B -->|Yes| C[Split into Channels]\n",
    "    B -->|No| D[Gray]-->H\n",
    "    C --> E[R]-->H\n",
    "    C --> F[G]-->H\n",
    "    C --> G[B]-->H\n",
    "    H{Multi Resolution?}-->|Yes|I[n Pyramid DownSampling Average]-->K\n",
    "    H-->|No|J[One Shot]-->K\n",
    "    K{algo} -->|HornShunck| M((.flo file))\n",
    "    K -->|LucasKanade| M\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    GT[Ground truth] --> B{BenchMark}\n",
    "    A(Flow Calculated) --> B\n",
    "    B --> AE(Angular Error and Std Dev)\n",
    "    B --> EE(Endpoint Error and Std Dev)\n",
    "```\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[import .flo file]-->C{Type}\n",
    "    C-->VF[Vector Field]\n",
    "    C-->MF[Middlebury Flow]\n",
    "    \n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nmcna\\AppData\\Local\\Temp\\ipykernel_24784\\3957010393.py:7: DeprecationWarning: Please use `convolve` from the `scipy.ndimage` namespace, the `scipy.ndimage.filters` namespace is deprecated.\n",
      "  from scipy.ndimage.filters import convolve as filter2\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import yaml\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "from scipy.ndimage.filters import convolve as filter2\n",
    "import struct\n",
    "import argparse\n",
    "import math\n",
    "from PIL import Image\n",
    "%matplotlib widget\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMport dataset with public ground truth\n",
    "%pip install wget\n",
    "import os\n",
    "import subprocess\n",
    "import wget\n",
    "import zipfile\n",
    "\n",
    "# Specify the URLs of the files to download\n",
    "url1 = \"https://vision.middlebury.edu/flow/data/comp/zip/other-gt-flow.zip\"\n",
    "url2 = \"https://vision.middlebury.edu/flow/data/comp/zip/other-color-allframes.zip\"\n",
    "url3 = \"https://vision.middlebury.edu/flow/data/comp/zip/eval-color-allframes.zip\"\n",
    "# Specify the destination folder to store the downloaded files\n",
    "destination_folder = \"dataset/\"\n",
    "\n",
    "# Download the files using wget\n",
    "file1 = wget.download(url1, out=destination_folder)\n",
    "file2 = wget.download(url2, out=destination_folder)\n",
    "file3 = wget.download(url3, out=destination_folder)\n",
    "\n",
    "# Unzip the files\n",
    "with zipfile.ZipFile(file1, 'r') as zip_ref:\n",
    "    zip_ref.extractall(destination_folder)\n",
    "\n",
    "with zipfile.ZipFile(file2, 'r') as zip_ref:\n",
    "    zip_ref.extractall(destination_folder)\n",
    "    \n",
    "with zipfile.ZipFile(file3, 'r') as zip_ref:\n",
    "    zip_ref.extractall(destination_folder)\n",
    "\n",
    "# Remove the zip files if needed\n",
    "os.remove(file1)\n",
    "os.remove(file2)\n",
    "os.remove(file3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all jpgs and pngs in a folder and returns a list of their respective path \n",
    "def get_frame_paths(folder_path):\n",
    "    frame_paths = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.jpg') or filename.endswith('.png'):\n",
    "            frame_path = os.path.join(folder_path, filename)\n",
    "            frame_paths.append(frame_path)\n",
    "    frame_paths.sort()\n",
    "    ##print(frame_paths)\n",
    "    return frame_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": [
     "Ingest"
    ]
   },
   "outputs": [],
   "source": [
    "# Checks if the inut file is a directory and if it is it will try and get the frames inside the folder\n",
    "# it will check if it is and mp4 and if it is it will read it and create a list of frames\n",
    "def get_input_frames(input_path):\n",
    "    frames = []\n",
    "    if os.path.isdir(input_path):\n",
    "        frame_paths = get_frame_paths(input_path)\n",
    "        #print(f\"{frame_paths}\")\n",
    "        frames = [cv2.imread(frame_path) for frame_path in frame_paths]\n",
    "    else :\n",
    "        #print(\"here\")\n",
    "        if input_path.endswith('.mp4') or input_path.endswith('.mov') :\n",
    "            cap = cv2.VideoCapture(input_path)\n",
    "            frames = []\n",
    "            while True:\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                frames.append(frame)\n",
    "            cap.release()\n",
    "    return frames"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Algo Config from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads a yaml file with the parameters to run the code\n",
    "def load_config(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        config['input_path']= os.getcwd()+config['input_path'] # Fix to run in the location it called\n",
    "        #print(config)\n",
    "    return config"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flo file format Aux Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "Flow"
    ]
   },
   "outputs": [],
   "source": [
    "\"\"\" import struct\n",
    "\n",
    "def write_flow_field(flow, filename):\n",
    "    height, width, _ = flow.shape\n",
    "    with open(filename, 'wb') as f:\n",
    "        # Write the magic number: 'PIEH' in ASCII\n",
    "        f.write(b'PIEH')\n",
    "\n",
    "        # Write the width and height of the flow field\n",
    "        f.write(np.array(width).astype(np.int32).tobytes())\n",
    "        f.write(np.array(height).astype(np.int32).tobytes())\n",
    "\n",
    "        # Interleave the u and v values and write the flow field data\n",
    "        flow_data = np.concatenate((flow[..., 0], flow[..., 1]), axis=1)\n",
    "        f.write(flow_data.astype(np.float32).tobytes())\n",
    "\n",
    "def read_flow_field(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        # Read the magic number\n",
    "        magic = f.read(4).decode()\n",
    "        if magic != 'PIEH':\n",
    "            raise ValueError('Invalid flow file format.')\n",
    "\n",
    "        # Read the width and height of the flow field\n",
    "        width = np.frombuffer(f.read(4), dtype=np.int32)[0]\n",
    "        height = np.frombuffer(f.read(4), dtype=np.int32)[0]\n",
    "\n",
    "        # Read the flow field data\n",
    "        flow_data = np.frombuffer(f.read(), dtype=np.float32)\n",
    "        flow = flow_data.reshape((height, width, 2))\n",
    "\n",
    "    return flow\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def read_flow_field(file_path):\n",
    "    with open(file_path, \"rb\") as f:\n",
    "        # Read .flo header\n",
    "        magic = struct.unpack('f', f.read(4))[0]\n",
    "        width = struct.unpack('i', f.read(4))[0]\n",
    "        height = struct.unpack('i', f.read(4))[0]\n",
    "\n",
    "        # Ensure .flo file format\n",
    "        assert magic == 202021.25, \"Invalid .flo file format.\"\n",
    "\n",
    "        # Read optical flow data\n",
    "        flow_data = np.fromfile(f, np.float32, width * height * 2)\n",
    "        flow_data = flow_data.reshape((height, width, 2))\n",
    "\n",
    "    return flow_data\n",
    "\n",
    "\n",
    "def write_flow_field(file_path, flow_data):\n",
    "    height, width = flow_data.shape[:2]\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        # Write .flo header\n",
    "        f.write(struct.pack('f', 202021.25))\n",
    "        f.write(struct.pack('i', width))\n",
    "        f.write(struct.pack('i', height))\n",
    "\n",
    "        # Write optical flow data\n",
    "        flow_data.flatten().astype(np.float32).tofile(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_angular_error(flow_gt, flow_pred):\n",
    "    flow_gt_norm = np.sqrt(np.sum(flow_gt ** 2, axis=2))\n",
    "    flow_pred_norm = np.sqrt(np.sum(flow_pred ** 2, axis=2))\n",
    "    dot_product = np.sum(flow_gt * flow_pred, axis=2)\n",
    "    cos_theta = np.clip(dot_product / (flow_gt_norm * flow_pred_norm), -1.0, 1.0)\n",
    "    angular_error = np.arccos(cos_theta)\n",
    "    return angular_error\n",
    "\n",
    "def calculate_endpoint_error(flow_gt, flow_pred):\n",
    "    endpoint_error = np.sqrt(np.sum((flow_gt - flow_pred) ** 2, axis=2))\n",
    "    return endpoint_error\n",
    "\n",
    "\n",
    "def calculate_error_statistics(error):\n",
    "    mean_error = np.mean(error)\n",
    "    std_error = np.std(error)\n",
    "    return mean_error, std_error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The original and new files are identical: True\n",
      "Mean Endpoint Error: 0.0\n",
      "Standard Deviation of Endpoint Error: 0.0\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "original_file = \"/Users/nmcna/Desktop/FEUP/4oano/2osemestre/VC/assignment2/dataset/other-gt-flow/Urban2/flow10.flo\"\n",
    "new_file = \"new.flo\"\n",
    "\n",
    "# Read the original .flo file\n",
    "original_flow = read_flow_field(original_file)\n",
    "\n",
    "# Write the original flow to a new .flo file\n",
    "write_flow_field(new_file, original_flow)\n",
    "\n",
    "# Read the new .flo file\n",
    "new_flow = read_flow_field(new_file)\n",
    "\n",
    "#Sanity check for write and read function\n",
    "\n",
    "# Compare the original and new flow files\n",
    "identical = np.array_equal(original_flow, new_flow)\n",
    "print(\"The original and new files are identical:\", identical)\n",
    "\n",
    "angular_error = calculate_angular_error(original_flow, new_flow)\n",
    "angular_mean, angular_std = calculate_error_statistics(angular_error)\n",
    "\n",
    "#print('Mean Angular Error:', angular_mean)\n",
    "#print('Standard Deviation of Angular Error:', angular_std)\n",
    "\n",
    "endpoint_error = calculate_endpoint_error(original_flow, new_flow)\n",
    "endpoint_mean, endpoint_std = calculate_error_statistics(endpoint_error)\n",
    "\n",
    "print('Mean Endpoint Error:', endpoint_mean)\n",
    "print('Standard Deviation of Endpoint Error:', endpoint_std)\n",
    "os.remove(new_file)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lucas-Kanade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLK_MC(frames, feature_params, lk_params, input_path):\n",
    "    color = np.random.randint(0, 255, (100, 3))\n",
    "    old_frame = frames[0]\n",
    "    #old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "    old_frameB, old_frameG, old_frameR = cv2.split(old_frame)\n",
    "    \n",
    "    p0B = cv2.goodFeaturesToTrack(old_frameB, mask=None, **feature_params)\n",
    "    p0G = cv2.goodFeaturesToTrack(old_frameG, mask=None, **feature_params)\n",
    "    p0R = cv2.goodFeaturesToTrack(old_frameR, mask=None, **feature_params)\n",
    "\n",
    "    maskB = np.zeros_like(old_frame)\n",
    "    maskG = np.zeros_like(old_frame)\n",
    "    maskR = np.zeros_like(old_frame)\n",
    "\n",
    "    for frame_num in frames[1:]:\n",
    "        \n",
    "        current_frame = frame_num\n",
    "        #frame_gray = cv2.cvtColor(frame_num, cv2.COLOR_BGR2GRAY)\n",
    "        current_frameB, current_frameG, current_frameR = cv2.split(frame_num)\n",
    "        \n",
    "        p1B, stB, err = cv2.calcOpticalFlowPyrLK(old_frameB, current_frameB, p0B, None, **lk_params)\n",
    "        p1G, stG, err = cv2.calcOpticalFlowPyrLK(old_frameG, current_frameG, p0G, None, **lk_params)\n",
    "        p1R, stR, err = cv2.calcOpticalFlowPyrLK(old_frameR, current_frameR, p0R, None, **lk_params)\n",
    "        \n",
    "        #print(\"p1: \", np.asarray(p1).shape)\n",
    "        #print(np.asarray(p1).shape)\n",
    "        #print(p1)\n",
    "        if p1B is not None:\n",
    "            \n",
    "            good_newB = p1B[stB==1]\n",
    "            good_oldB = p0B[stB==1]\n",
    "        if p1G is not None:\n",
    "            \n",
    "            good_newG = p1G[stG==1]\n",
    "            good_oldG = p0G[stG==1]\n",
    "        if p1R is not None:\n",
    "            \n",
    "            good_newR = p1R[stR==1]\n",
    "            good_oldR = p0R[stR==1]\n",
    "        \n",
    "        for i, (new, old) in enumerate(zip(good_newB, good_oldB)):\n",
    "            aB, bB = new.ravel()\n",
    "            cB, dB = old.ravel()\n",
    "            maskB = cv2.line(maskB, (int(aB), int(bB)), (int(cB), int(dB)), color[i].tolist(), 2)\n",
    "        for i, (new, old) in enumerate(zip(good_newG, good_oldG)):\n",
    "            aG, bG = new.ravel()\n",
    "            cG, dG = old.ravel()\n",
    "            maskG = cv2.line(maskG, (int(aG), int(bG)), (int(cG), int(dG)), color[i].tolist(), 2)\n",
    "        for i, (new, old) in enumerate(zip(good_newR, good_oldR)):\n",
    "            aR, bR = new.ravel()\n",
    "            cR, dR = old.ravel()\n",
    "            maskR = cv2.line(maskR, (int(aR), int(bR)), (int(cR), int(dR)), color[i].tolist(), 2)\n",
    "        \n",
    "        # Calculate the displacement vectors\n",
    "        flowB = good_newB - good_oldB\n",
    "        flowG = good_newG - good_oldG\n",
    "        flowR = good_newR - good_oldR\n",
    "\n",
    "        # Prepare the flow matrix\n",
    "        flow_dataB = np.zeros((current_frame.shape[0], current_frame.shape[1], 2), dtype=np.float32)\n",
    "        valid_indicesB = np.where((good_oldB[:, 0] >= 0) & (good_oldB[:, 0] < current_frame.shape[1]) &\n",
    "                                (good_oldB[:, 1] >= 0) & (good_oldB[:, 1] < current_frame.shape[0]))\n",
    "        \n",
    "        flow_dataG = np.zeros((current_frame.shape[0], current_frame.shape[1], 2), dtype=np.float32)\n",
    "        valid_indicesG = np.where((good_oldG[:, 0] >= 0) & (good_oldG[:, 0] < current_frame.shape[1]) &\n",
    "                                (good_oldG[:, 1] >= 0) & (good_oldG[:, 1] < current_frame.shape[0]))\n",
    "        \n",
    "        flow_dataR = np.zeros((current_frame.shape[0], current_frame.shape[1], 2), dtype=np.float32)\n",
    "        valid_indicesR = np.where((good_oldR[:, 0] >= 0) & (good_oldR[:, 0] < current_frame.shape[1]) &\n",
    "                                (good_oldR[:, 1] >= 0) & (good_oldR[:, 1] < current_frame.shape[0]))\n",
    "\n",
    "        # Update flow matrix with valid keypoints and their displacement vectors\n",
    "        valid_good_oldB = good_oldB[valid_indicesB].astype(int)\n",
    "        flow_dataB[valid_good_oldB[:, 1], valid_good_oldB[:, 0]] = flowB.squeeze()[valid_indicesB]\n",
    "        \n",
    "        valid_good_oldG = good_oldG[valid_indicesG].astype(int)\n",
    "        flow_dataG[valid_good_oldG[:, 1], valid_good_oldG[:, 0]] = flowG.squeeze()[valid_indicesG]\n",
    "        \n",
    "        valid_good_oldR = good_oldR[valid_indicesR].astype(int)\n",
    "        flow_dataR[valid_good_oldR[:, 1], valid_good_oldR[:, 0]] = flowR.squeeze()[valid_indicesR]\n",
    "\n",
    "        frame_with_flowB = cv2.add(current_frame, maskB)\n",
    "        old_frameB = current_frameB.copy()\n",
    "        p0B = good_newB.reshape(-1,1,2)\n",
    "        \n",
    "        frame_with_flowG = cv2.add(current_frame, maskG)\n",
    "        old_frameG = current_frameG.copy()\n",
    "        p0G = good_newG.reshape(-1,1,2)\n",
    "        \n",
    "        frame_with_flowR = cv2.add(current_frame, maskR)\n",
    "        old_frameR = current_frameR.copy()\n",
    "        p0R = good_newR.reshape(-1,1,2)\n",
    "        \n",
    "    if os.path.isfile(input_path):\n",
    "        file_name, file_extension = os.path.splitext(input_path)\n",
    "        if file_extension.lower() not in ['.png', '.jpg']:\n",
    "            #print(\"File name without extension:\", os.path.basename(file_name))\n",
    "            cv2.imwrite(file_name+'_flow_B.png', frame_with_flowB)\n",
    "            cv2.imwrite(file_name+'_flow_G.png', frame_with_flowG)\n",
    "            cv2.imwrite(file_name+'_flow_R.png', frame_with_flowR)\n",
    "            #write_flow_field(file_name+'_flow_field.flo',mask)\n",
    "            # Save flow data to .flo file\n",
    "            cv2.writeOpticalFlow(file_name+'_flow_field_B.flo', flow_dataB)\n",
    "            cv2.writeOpticalFlow(file_name+'_flow_field_G.flo', flow_dataG)\n",
    "            cv2.writeOpticalFlow(file_name+'_flow_field_R.flo', flow_dataR)\n",
    "        else:\n",
    "            print(\"The file extension is .png or .jpg\")\n",
    "    else:\n",
    "        directory = input_path.split('dataset\\\\')[1].split('\\\\')\n",
    "        #print(\"Directory after 'dataset\\\\':\", directory[1])\n",
    "        cv2.imwrite(directory[1]+'_flow_B.png', frame_with_flowB)\n",
    "        cv2.imwrite(directory[1]+'_flow_G.png', frame_with_flowG)\n",
    "        cv2.imwrite(directory[1]+'_flow_R.png', frame_with_flowR)\n",
    "        write_flow_field(maskB,directory[1]+'_flow_field_B.flo')\n",
    "        write_flow_field(maskG,directory[1]+'_flow_field_G.flo')\n",
    "        write_flow_field(maskR,directory[1]+'_flow_field_R.flo')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeLK(frames, feature_params, lk_params, input_path):\n",
    "    \n",
    "    color = np.random.randint(0, 255, (100, 3))\n",
    "    old_frame = frames[0]\n",
    "    old_gray = cv2.cvtColor(old_frame, cv2.COLOR_BGR2GRAY)\n",
    "    p0 = cv2.goodFeaturesToTrack(old_gray, mask=None, **feature_params)\n",
    "\n",
    "    mask = np.zeros_like(old_frame)\n",
    "    height, width, channels = old_frame.shape\n",
    "    mask_flow = np.zeros((height,width,2))\n",
    "    #print(\"DIM: \", mask_flow.shape)\n",
    "\n",
    "    for frame_num in frames[1:]:\n",
    "        \n",
    "        current_frame = frame_num\n",
    "        frame_gray = cv2.cvtColor(frame_num, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        p1, st, err = cv2.calcOpticalFlowPyrLK(old_gray, frame_gray, p0, None, **lk_params)\n",
    "        #print(\"p1: \", np.asarray(p1).shape)\n",
    "        #print(np.asarray(p1).shape)\n",
    "        #print(p1)\n",
    "        if p1 is not None:\n",
    "            \n",
    "            good_new = p1[st==1]\n",
    "            good_old = p0[st==1]\n",
    "        \n",
    "        for i, (new, old) in enumerate(zip(good_new, good_old)):\n",
    "            a, b = new.ravel()\n",
    "            c, d = old.ravel()\n",
    "            mask = cv2.line(mask, (int(a), int(b)), (int(c), int(d)), color[i].tolist(), 2)\n",
    "        \n",
    "        # Calculate the displacement vectors\n",
    "        flow = good_new - good_old\n",
    "\n",
    "        # Prepare the flow matrix\n",
    "        flow_data = np.zeros((frame_gray.shape[0], frame_gray.shape[1], 2), dtype=np.float32)\n",
    "        valid_indices = np.where((good_old[:, 0] >= 0) & (good_old[:, 0] < frame_gray.shape[1]) &\n",
    "                                (good_old[:, 1] >= 0) & (good_old[:, 1] < frame_gray.shape[0]))\n",
    "\n",
    "        # Update flow matrix with valid keypoints and their displacement vectors\n",
    "        valid_good_old = good_old[valid_indices].astype(int)\n",
    "        flow_data[valid_good_old[:, 1], valid_good_old[:, 0]] = flow.squeeze()[valid_indices]\n",
    "\n",
    "    \n",
    "\n",
    "        frame_with_flow = cv2.add(current_frame, mask)\n",
    "        old_gray = frame_gray.copy()\n",
    "        p0 = good_new.reshape(-1,1,2)\n",
    "        \n",
    "    if os.path.isfile(input_path):\n",
    "        file_name, file_extension = os.path.splitext(input_path)\n",
    "        if file_extension.lower() not in ['.png', '.jpg']:\n",
    "            #print(\"File name without extension:\", os.path.basename(file_name))\n",
    "            cv2.imwrite(file_name+'_flow.png', frame_with_flow)\n",
    "            #write_flow_field(file_name+'_flow_field.flo',mask)\n",
    "            # Save flow data to .flo file\n",
    "            cv2.writeOpticalFlow(file_name+'_flow_field.flo', flow_data)\n",
    "        else:\n",
    "            print(\"The file extension is .png or .jpg\")\n",
    "    else:\n",
    "        directory = input_path.split('dataset\\\\')[1].split('\\\\')\n",
    "        #print(\"Directory after 'dataset\\\\':\", directory[1])\n",
    "        cv2.imwrite(directory[1]+'_flow.png', frame_with_flow)\n",
    "        write_flow_field(mask,directory[1]+'_flow_field.flo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Loading File \\'config_LK.yml\\'\")\n",
    "\n",
    "config = load_config('config_LK.yml')\n",
    "\n",
    "input_path = config['input_path']\n",
    "\n",
    "frames = get_input_frames(config['input_path'])\n",
    "\n",
    "#print(np.asarray(frames).shape)\n",
    "\n",
    "feature_params = dict(\n",
    "    maxCorners=config['max_corners'],\n",
    "    qualityLevel=config['quality_level'],\n",
    "    minDistance=config['min_distance'],\n",
    "    blockSize=config['block_size']\n",
    ")\n",
    "lk_params = dict(\n",
    "    winSize=(config['window_size'], config['window_size']),\n",
    "    maxLevel=config['max_level'],\n",
    "    criteria=(cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, config['max_iterations'], config['epsilon'])\n",
    ")\n",
    "\n",
    "#computeLK(frames, feature_params, lk_params, input_path)\n",
    "computeLK_MC(frames, feature_params, lk_params, input_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread('dataset/other-data/Urban2/frame07.png')\n",
    "\n",
    "flow = read_flow_field('dataset/other-gt-flow/Urban2/flow10.flo')\n",
    "\n",
    "warped = cv2.remap(image, flow, None, cv2.INTER_LINEAR)\n",
    "\n",
    "cv2.imshow('warp', warped)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Horn-Shunchk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeHS_MC(path, name1, name2, alpha, delta):\n",
    "    \n",
    "    print(\"Computing using: \", name1, \" and \", name2)\n",
    "\n",
    "    beforeImg1 = cv2.imread(os.path.join(path, name1), cv2.IMREAD_GRAYSCALE).astype(float)\n",
    "    afterImg = cv2.imread(os.path.join(path, name2), cv2.IMREAD_GRAYSCALE).astype(float)\n",
    "\n",
    "    if beforeImg is None:\n",
    "        raise NameError(\"Can't find image: \\\"\" + name1 + '\\\"')\n",
    "    elif afterImg is None:\n",
    "        raise NameError(\"Can't find image: \\\"\" + name2 + '\\\"')\n",
    "\n",
    "    #removing noise\n",
    "    beforeImg  = cv2.GaussianBlur(beforeImg1, (5, 5), 0)\n",
    "    afterImg = cv2.GaussianBlur(afterImg, (5, 5), 0)\n",
    "\n",
    "\n",
    "    u_list = []\n",
    "    v_list = []\n",
    "    for i in range(3): # 3channels if this works im dumb\n",
    "        # set up initial values\n",
    "        u = np.zeros((beforeImg.shape[0], beforeImg.shape[1]))\n",
    "        v = np.zeros((beforeImg.shape[0], beforeImg.shape[1]))\n",
    "        fx, fy, ft = get_derivatives(beforeImg[:,:,i], afterImg[:,:,i])\n",
    "        avg_kernel = np.array([[1 / 12, 1 / 6, 1 / 12],\n",
    "                                [1 / 6, 0, 1 / 6],\n",
    "                                [1 / 12, 1 / 6, 1 / 12]], float)\n",
    "        iter_counter = 0\n",
    "        while True:\n",
    "            iter_counter += 1\n",
    "            u_avg = filter2(u, avg_kernel)\n",
    "            v_avg = filter2(v, avg_kernel)\n",
    "            p = fx * u_avg + fy * v_avg + ft\n",
    "            d = 4 * alpha**2 + fx**2 + fy**2\n",
    "            prev = u\n",
    "\n",
    "            u = u_avg - fx * (p / d)\n",
    "            v = v_avg - fy * (p / d)\n",
    "\n",
    "            diff = np.linalg.norm(u - prev, 2)\n",
    "            #converges check (at most 300 iterations)\n",
    "            if  diff < delta or iter_counter > 300:\n",
    "                # #print(\"iteration number: \", iter_counter)\n",
    "                break\n",
    "        u_list.append(u)\n",
    "        v_list.append(v)\n",
    "\n",
    "\n",
    "    u = np.mean(u_list)\n",
    "    v = np.mean(v_list)  \n",
    "        \n",
    "    draw_quiver(u, v, beforeImg1, name1, name2)\n",
    "\n",
    "    return [u, v]\n",
    "\n",
    "def computeHS(path, name1, name2, alpha, delta):\n",
    "    \n",
    "    print(\"Computing using: \", name1, \" and \", name2)\n",
    "    beforeImg = cv2.imread(os.path.join(path, name1), cv2.IMREAD_GRAYSCALE)\n",
    "    afterImg = cv2.imread(os.path.join(path, name2), cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    if beforeImg is None:\n",
    "        raise NameError(\"Can't find image: \\\"\" + name1 + '\\\"')\n",
    "    elif afterImg is None:\n",
    "        raise NameError(\"Can't find image: \\\"\" + name2 + '\\\"')\n",
    "\n",
    "    beforeImg = cv2.imread(os.path.join(path, name1), cv2.IMREAD_GRAYSCALE).astype(float)\n",
    "    afterImg = cv2.imread(os.path.join(path, name2), cv2.IMREAD_GRAYSCALE).astype(float)\n",
    "\n",
    "    #removing noise\n",
    "    beforeImg  = cv2.GaussianBlur(beforeImg, (5, 5), 0)\n",
    "    afterImg = cv2.GaussianBlur(afterImg, (5, 5), 0)\n",
    "\n",
    "    # set up initial values\n",
    "    u = np.zeros((beforeImg.shape[0], beforeImg.shape[1]))\n",
    "    v = np.zeros((beforeImg.shape[0], beforeImg.shape[1]))\n",
    "    fx, fy, ft = get_derivatives(beforeImg, afterImg)\n",
    "    avg_kernel = np.array([[1 / 12, 1 / 6, 1 / 12],\n",
    "                            [1 / 6, 0, 1 / 6],\n",
    "                            [1 / 12, 1 / 6, 1 / 12]], float)\n",
    "    iter_counter = 0\n",
    "    while True:\n",
    "        iter_counter += 1\n",
    "        u_avg = filter2(u, avg_kernel)\n",
    "        v_avg = filter2(v, avg_kernel)\n",
    "        p = fx * u_avg + fy * v_avg + ft\n",
    "        d = 4 * alpha**2 + fx**2 + fy**2\n",
    "        prev = u\n",
    "\n",
    "        u = u_avg - fx * (p / d)\n",
    "        v = v_avg - fy * (p / d)\n",
    "\n",
    "        diff = np.linalg.norm(u - prev, 2)\n",
    "        #converges check (at most 300 iterations)\n",
    "        if  diff < delta or iter_counter > 300:\n",
    "            # #print(\"iteration number: \", iter_counter)\n",
    "            break\n",
    "\n",
    "    draw_quiver(u, v, beforeImg, name1, name2)\n",
    "\n",
    "    \n",
    "    return [u, v]\n",
    "\n",
    "def show_image(name, image):\n",
    "    print(\"take this image!!!\")\n",
    "    if image is None:\n",
    "        return\n",
    "\n",
    "    cv2.imshow(name, image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "def get_magnitude(u, v):\n",
    "    print(\"getting magnitude...\")\n",
    "    scale = 3\n",
    "    sum = 0.0\n",
    "    counter = 0.0\n",
    "\n",
    "    for i in range(0, u.shape[0], 8):\n",
    "        for j in range(0, u.shape[1],8):\n",
    "            counter += 1\n",
    "            dy = v[i,j] * scale\n",
    "            dx = u[i,j] * scale\n",
    "            magnitude = (dx**2 + dy**2)**0.5\n",
    "            sum += magnitude\n",
    "\n",
    "    mag_avg = sum / counter\n",
    "\n",
    "    return mag_avg\n",
    "\n",
    "def draw_quiver(u,v,beforeImg,name1, name2):\n",
    "    print(\"drawing...\")\n",
    "    scale = 3\n",
    "    ax = plt.figure().gca()\n",
    "    ax.imshow(beforeImg, cmap = 'gray')\n",
    "\n",
    "    magnitudeAvg = get_magnitude(u, v)\n",
    "\n",
    "    for i in range(0, u.shape[0], 8):\n",
    "        for j in range(0, u.shape[1],8):\n",
    "            dy = v[i,j] * scale\n",
    "            dx = u[i,j] * scale\n",
    "            magnitude = (dx**2 + dy**2)**0.5\n",
    "            #draw only significant changes\n",
    "            if magnitude > magnitudeAvg:\n",
    "                ax.arrow(j,i, dx, dy, color = 'red')\n",
    "\n",
    "    plt.draw()\n",
    "    file1 = os.path.splitext(name1)[0]\n",
    "    file2 = os.path.splitext(name2)[0]\n",
    "    plt.savefig('/workspace/VC/assignment2/output/'+file1+file2+'.png')\n",
    "    plt.show()\n",
    "    \n",
    "def get_derivatives(img1, img2):\n",
    "    print(\"derivating...\")\n",
    "    #derivative masks\n",
    "    x_kernel = np.array([[-1, 1], [-1, 1]]) * 0.25\n",
    "    y_kernel = np.array([[-1, -1], [1, 1]]) * 0.25\n",
    "    t_kernel = np.ones((2, 2)) * 0.25\n",
    "\n",
    "    fx = filter2(img1,x_kernel) + filter2(img2,x_kernel)\n",
    "    fy = filter2(img1, y_kernel) + filter2(img2, y_kernel)\n",
    "    ft = filter2(img1, -t_kernel) + filter2(img2, t_kernel)\n",
    "\n",
    "    return [fx,fy, ft]\n",
    "\n",
    "def calculate_aee(u, v, ground_truth_u, ground_truth_v):\n",
    "    error = np.sqrt((u - ground_truth_u)**2 + (v - ground_truth_v)**2)\n",
    "    aee = np.mean(error)\n",
    "    return aee\n",
    "def calculate_aee_std(u, v, ground_truth_u, ground_truth_v):\n",
    "    error = np.sqrt((u - ground_truth_u)**2 + (v - ground_truth_v)**2)\n",
    "    aee_std = np.std(error)\n",
    "    return aee_std\n",
    "def calculate_aae(u, v, ground_truth_u, ground_truth_v):\n",
    "    error_cos = (u * ground_truth_u + v * ground_truth_v +\n",
    "                 1.0) / (np.sqrt(u**2 + v**2 + 1.0) *\n",
    "                         np.sqrt(ground_truth_u**2 + ground_truth_v**2 + 1.0))\n",
    "    error_cos = np.clip(error_cos, -1.0, 1.0)  # Clip values to ensure valid range\n",
    "    angular_error = np.arccos(error_cos)  # Calculate angular error in radians\n",
    "    aae = np.degrees(np.mean(angular_error))  # Convert to degrees and calculate the mean\n",
    "    return aae\n",
    "\n",
    "def calculate_aae_std(u, v, ground_truth_u, ground_truth_v):\n",
    "    error_cos = (u * ground_truth_u + v * ground_truth_v +\n",
    "                 1.0) / (np.sqrt(u**2 + v**2 + 1.0) *\n",
    "                         np.sqrt(ground_truth_u**2 + ground_truth_v**2 + 1.0))\n",
    "    error_cos = np.clip(error_cos, -1.0, 1.0)  # Clip values to ensure valid range\n",
    "    angular_error = np.arccos(error_cos)  # Calculate angular error in radians\n",
    "    aae_std = np.degrees(np.std(angular_error))  # Convert to degrees and calculate the standard deviation\n",
    "    return aae_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(f\"Loading File \\'config_HS.yml\\'\")\n",
    "\n",
    "config = load_config('config_HS.yml')\n",
    "\n",
    "input_path = config['input_path']\n",
    "\n",
    "gt_path = '/workspace/VC/assignment2/dataset/other-gt-flow/Urban2/flow10.flo'\n",
    "\n",
    "gt = read_flow_field(gt_path)\n",
    "\n",
    "gt_u = gt[:, :, 0].astype(float) / 255.0  # Normalize u component to [0, 1]\n",
    "gt_v = gt[:, :, 1].astype(float) / 255.0  # Normalize v component to [0, 1]\n",
    "#name = get_frame_paths(input_path)\n",
    "frames = os.listdir(input_path)\n",
    "##print(frames)\n",
    "aee_values = []\n",
    "aee_std_values = []\n",
    "aae_values = []\n",
    "aae_std_values = []\n",
    "for i in range(0,len(frames)-1):\n",
    "    #print(\"CYCLE: \", i)\n",
    "    u, v = computeHS_MC(input_path, frames[i], frames[i+1], alpha=15, delta=10**-1)\n",
    "    \n",
    "    aee = calculate_aee(u, v, gt_u, gt_v)\n",
    "    aee_std = calculate_aee_std(u, v, gt_u, gt_v)\n",
    "    print(\"Average End-Point Error: \", aee)\n",
    "    print(\"Standard Deviation: \", aee_std)\n",
    "    aee_values.append(aee)\n",
    "    aee_std_values.append(aee_std)\n",
    "    \n",
    "    aae = calculate_aae(u, v, gt_u, gt_v)\n",
    "    aae_std = calculate_aae_std(u, v, gt_u, gt_v)\n",
    "    print(\"Average Angular Error: \", aae)\n",
    "    print(\"Standard Deviation: \", aae_std)\n",
    "    aae_values.append(aae)\n",
    "    aae_std_values.append(aae_std)\n",
    "    \n",
    "print(\"MEAN AEE: \", np.mean(aee_values))\n",
    "print(\"MEAN STD: \", np.mean(aee_std_values))\n",
    "print(\"MEAN AAE: \", np.mean(aae_values))\n",
    "print(\"MEAN STD: \", np.mean(aae_std_values))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done Using [flowvid](https://github.com/diegoroyo/flowvid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import flowvid\n",
    "import numpy as np\n",
    "from IPython.display import HTML\n",
    "\n",
    "import flowvid as fv\n",
    "\n",
    "flo_data = fv.input.flo('/workspace/VC/assignment2/dataset/slow_traffic_small_flow_field.flo')\n",
    "\n",
    "# You can normalize by frame OR the whole video\n",
    "# Normalize each flo file independently\n",
    "flo_frame = fv.normalize_frame(flo_data)\n",
    "# Normalize all flo files at once, applying a clamp/gamma curve\n",
    "flo_video = fv.normalize_video(flo_data, clamp_pct=0.8, gamma=1.5)\n",
    "\n",
    "# Conversion from flow data to RGB\n",
    "rgb_frames = fv.flow_to_rgb(flo_video)\n",
    "\n",
    "out5 = fv.output.show_plot(title='Flow colors', framerate=10)\n",
    "out5.show_all(rgb_frames, show_count=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "def create_gif_from_folder(folder_path, duration=100, loop=0):\n",
    "    gif_file = os.path.join(os.getcwd()+'/gifs/', f\"{os.path.basename(folder_path)}.gif\")\n",
    "    #print(gif_file)\n",
    "    images = []\n",
    "    png_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.png')])\n",
    "\n",
    "    for file in png_files:\n",
    "        image_path = os.path.join(folder_path, file)\n",
    "        image = Image.open(image_path)\n",
    "        images.append(image)\n",
    "\n",
    "    images[0].save(gif_file, save_all=True, append_images=images[1:], format='GIF', duration=duration, loop=loop)\n",
    "    return gif_file\n",
    "\n",
    "def gif_to_html_image(gif_file):\n",
    "    with open(gif_file, 'rb') as f:\n",
    "        gif_data = f.read()\n",
    "\n",
    "    base64_data = base64.b64encode(gif_data).decode('utf-8')\n",
    "    html_image = f'<img src=\"data:image/gif;base64,{base64_data}\" />'\n",
    "\n",
    "    return html_image\n",
    "\n",
    "\n",
    "# Example usage\n",
    "#folder_path = '/workspace/VC/assignment2/dataset/other-data/Urban2'\n",
    "#create_gif_from_folder(folder_path, duration=200, loop=3)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bench Marking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Not Finished unfortunately :(\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "\n",
    "def find_common_folders(folder1, folder2):\n",
    "    folders1 = set(os.listdir(os.getcwd()+folder1))\n",
    "    folders2 = set(os.listdir(os.getcwd()+folder2))\n",
    "    common_folders = list(folders1.intersection(folders2))\n",
    "    print(f\"Folders that have a ground truth to evaluate:{common_folders}\")\n",
    "    return common_folders\n",
    "\n",
    "\n",
    "\n",
    "def display_results():\n",
    "    # Create an empty dataframe to store the results\n",
    "\n",
    "    paths_to_evalutate = find_common_folders('/dataset/other-data/','/dataset/other-gt-flow/')\n",
    "\n",
    "    lk_df = pd.DataFrame(columns=['Name','Source', 'Processed', 'Average Angular Error','Std Angular Error', 'Average Endpoint Error','Std Endpoint Error'])\n",
    "    hs_df = pd.DataFrame(columns=['Name','Source', 'Processed', 'Average Angular Error','Std Angular Error', 'Average Endpoint Error','Std Endpoint Error'])\n",
    "    \n",
    "    # Iterate through the function calls and store the results\n",
    "    for i, path, in enumerate(paths_to_evalutate):\n",
    "        source_path = os.getcwd()+'/dataset/other-data/'+path\n",
    "        # Add the function name and result to the dataframe\n",
    "        #source_gif = f\"![source]({create_gif_from_folder(source_path, duration=100)})\"\n",
    "        source_gif = gif_to_html_image(create_gif_from_folder(source_path, duration=100))\n",
    "        lk_df.loc[i+1] = [path,source_gif,'yes','30days',source_gif,'yes','30days']\n",
    "        hs_df.loc[i+1] = [path,source_gif,'yes','30days',source_gif,'yes','30days']\n",
    "\n",
    "    #Display the results as a markdown table\n",
    "    display(Markdown('# LucasKanade Results\\n\\n'+lk_df.to_markdown(index=False)+'\\n\\n\\n# HornShunck Results\\n\\n'+hs_df.to_markdown(index=False)))\n",
    "\n",
    "display_results()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
